{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99f9c70",
   "metadata": {},
   "source": [
    "# Neural Network w/ backpropagation in Python from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e7df8",
   "metadata": {},
   "source": [
    "Lecture: https://www.youtube.com/watch?v=59Hbtz7XgjM\n",
    "Post: https://cs231n.github.io/optimization-2/\n",
    "Post: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "Video: https://www.youtube.com/watch?v=4shguqlkTDM\n",
    "Code Inspiration: https://github.com/yacineMahdid/artificial-intelligence-and-machine-learning/blob/master/deep-learning-from-scratch-python/multi_layer_perceptron.ipynb (different data, try out different activations - sigmoid, ReLu, tanh)\n",
    "Code Inspiration 2: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdc3f8",
   "metadata": {},
   "source": [
    "### Functional Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2379b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "89c4f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR\n",
    "inp1 = [[0,0], [0,1], [1,0], [1,1]]\n",
    "out1 = [0,1,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "1d078e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.0424649763541316, 0.7275132307972553, 0.2817894641574412, 0.0153838518846936, 0.92494042436315]}, {'params': [0.3038116314700229, 0.4938206162882226, 0.5733637314974005, 0.7677368136591562, 0.04837722371730124]}, {'type': 'hidden'}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.40808710893356437, 0.2906956521598103]}, {'type': 'output'}]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize network with weights\n",
    "\n",
    "data_len = len(inp1)\n",
    "network = []\n",
    "n_layers = 2\n",
    "n_hidden = n_layers - 1\n",
    "n_neurons = [2, 1] # layer 1, layer 2, ... , layer n\n",
    "n_weights = [data_len, 1] # layer 1, layer 2, ... , layer n. Bias not included\n",
    "\n",
    "\n",
    "def init_weights(n_weights):\n",
    "    return np.random.rand(n_weights)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    l = [{'params': [np.random.rand() for n in range(n_weights[layer] + 1)]} for n in range(n_neurons[layer])]\n",
    "    network.append(l)\n",
    "\n",
    "i = 0\n",
    "for layer in network:\n",
    "    if i < n_hidden:\n",
    "        layer.append({'type': 'hidden'})\n",
    "    else:\n",
    "        layer.append({'type': 'output'})\n",
    "    i += 1\n",
    "\n",
    "def print_layers(network):\n",
    "    i = 1\n",
    "    for layer in network:\n",
    "        if layer[-1]['type'] == 'hidden':\n",
    "            print(f'HIDDEN LAYER {i}')\n",
    "        else:\n",
    "            print('OUTPUT LAYER')\n",
    "        print(layer)\n",
    "        print(' ')\n",
    "        i += 1\n",
    "\n",
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a1e9502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.0424649763541316, 0.7275132307972553, 0.2817894641574412, 0.0153838518846936, 0.92494042436315], 'output': 1.9650108230872338, 'activated': 1.9650108230872338}, {'params': [0.3038116314700229, 0.4938206162882226, 0.5733637314974005, 0.7677368136591562, 0.04837722371730124], 'output': 2.651035198821237, 'activated': 2.651035198821237}, {'type': 'hidden'}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.40808710893356437, 0.2906956521598103], 'output': 1.0925912379766434, 'activated': 0.7488693544926488}, {'type': 'output'}]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 2. Forward propagate\n",
    "\n",
    "# Calculates the output of a single neuron -> (weights * inputs) + bias\n",
    "def calc_neuron_output(params, inputs):\n",
    "    bias = params[-1]\n",
    "    output = bias\n",
    "    for i in range(len(params) - 1):\n",
    "        if any(isinstance(inp, list) for inp in inputs):\n",
    "            for inp in inputs[i]:\n",
    "                output += params[i] * inp\n",
    "        else:\n",
    "            output += params[i] * inputs[i]\n",
    "    return output\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(output):\n",
    "    return 1.0 / (1.0 + np.exp(-output))\n",
    "\n",
    "def ReLu(output):\n",
    "    return max(0, output)\n",
    "\n",
    "def tanh(output):\n",
    "    return (np.exp(output)-np.exp(-output))/(np.exp(output)+np.exp(-output))\n",
    "\n",
    "activation_functions = ('sigmoid', 'ReLu', 'tanh')\n",
    "\n",
    "def forward_propagate(inputs, activation_func):\n",
    "    inp = inputs\n",
    "    \n",
    "    if activation_func.__name__ not in activation_functions:\n",
    "        raise NameError('The activation function specified does not exist')\n",
    "    \n",
    "    for layer in network:\n",
    "        outs = []\n",
    "        for neuron in layer:\n",
    "            if 'type' in neuron.keys():\n",
    "                pass\n",
    "            else:\n",
    "                neuron_out = calc_neuron_output(neuron['params'], inp)\n",
    "                neuron['output'] = neuron_out\n",
    "                if layer[-1]['type'] == 'hidden':\n",
    "                    neuron['activated'] = activation_func(neuron['output'])\n",
    "                    outs.append(neuron['activated'])\n",
    "                else:\n",
    "                    neuron['activated'] = sigmoid(neuron['output']) # activation is output layer is sigmoid\n",
    "        inp = outs\n",
    "        \n",
    "    return activation_func # to be used in backpropagation for the derivative\n",
    "\n",
    "forward_propagate(inp1, ReLu)\n",
    "\n",
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2de8cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Back propagate error\n",
    "\n",
    "# Derivatives of activation functions\n",
    "def d_sigmoid(s):\n",
    "    return s*(1-s)\n",
    "\n",
    "def d_ReLu(r):\n",
    "    return 1 if r > 0 else 0\n",
    "\n",
    "def d_tanh(t):\n",
    "    return 1-t**2\n",
    "\n",
    "def backpropagate_neuron(network, i, expected_output):\n",
    "    \n",
    "    # Base case\n",
    "    if network[i][-1]['type'] == 'output':\n",
    "        value_to_pass = None\n",
    "        \n",
    "        for neuron in network[i]:\n",
    "            if 'type' not in neuron.keys():\n",
    "                actual_output = neuron['output']\n",
    "                error = actual_output - expected_output\n",
    "                value_to_pass = (actual_output, error)\n",
    "\n",
    "        return value_to_pass\n",
    "    # End of base case\n",
    "\n",
    "    actual_output, error = backpropagate_neuron(network, i + 1, expected_output)\n",
    "    \n",
    "    for neuron in network[i]:\n",
    "        if 'type' not in neuron.keys():\n",
    "            #### HERE - DELTA CALCULATION\n",
    "            neuron['delta'] = actual_output\n",
    "    \n",
    "\n",
    "backpropagate_neuron(network, 0, out1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "82e11279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.0424649763541316, 0.7275132307972553, 0.2817894641574412, 0.0153838518846936, 0.92494042436315], 'output': 1.9650108230872338, 'activated': 1.9650108230872338, 'delta': 1.0925912379766434}, {'params': [0.3038116314700229, 0.4938206162882226, 0.5733637314974005, 0.7677368136591562, 0.04837722371730124], 'output': 2.651035198821237, 'activated': 2.651035198821237, 'delta': 1.0925912379766434}, {'type': 'hidden'}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.40808710893356437, 0.2906956521598103], 'output': 1.0925912379766434, 'activated': 0.7488693544926488}, {'type': 'output'}]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# 4. Train network\n",
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Predict on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Predict on real world dataset - https://www.kaggle.com/datasets/whenamancodes/fraud-detection?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6132ca9b",
   "metadata": {},
   "source": [
    "### OOP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e79dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
