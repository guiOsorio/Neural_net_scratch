{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a11a4d4a",
   "metadata": {},
   "source": [
    "# Neural Network w/ backpropagation in Python from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432a4860",
   "metadata": {},
   "source": [
    "Lecture: https://www.youtube.com/watch?v=59Hbtz7XgjM\n",
    "Post: https://cs231n.github.io/optimization-2/\n",
    "Post: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "Video: https://www.youtube.com/watch?v=4shguqlkTDM\n",
    "Code Inspiration: https://github.com/yacineMahdid/artificial-intelligence-and-machine-learning/blob/master/deep-learning-from-scratch-python/multi_layer_perceptron.ipynb (different data, try out different activations - sigmoid, ReLu, tanh)\n",
    "Code Inspiration 2: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcb12d",
   "metadata": {},
   "source": [
    "### Functional Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60059532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee035d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [[2.7810836,2.550537003, 0],\n",
    "           [1.465489372,2.362125076, 0],\n",
    "           [3.396561688,4.400293529, 0],\n",
    "           [1.38807019,1.850220317, 0],\n",
    "           [3.06407232,3.005305973, 0],\n",
    "           [7.627531214,2.759262235, 1],\n",
    "           [5.332441248,2.088626775, 1],\n",
    "           [6.922596716,1.77106367, 1],\n",
    "           [8.675418651,-0.242068655, 1],\n",
    "           [7.673756466,3.508563011, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5532f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.3250176978548759, 0.7263017282699172, 0.04944697288034661]}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.5172413807841587, 0.9794618021114261]}, {'params': [0.8021491353998991, 0.5019634378645447]}]\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize network with weights\n",
    "\n",
    "n_hidden = 1 # number of hidden layers\n",
    "n_inputs = len(train[0][:-1]) # number of features\n",
    "n_hidden_neurons = [1, 1] # number of neurons in hidden layer\n",
    "n_outputs = 2 # number of possible outputs to be predicted\n",
    "\n",
    "def init_network(n_inputs, n_hidden, n_hidden_neurons, n_outputs):\n",
    "\n",
    "    network = []\n",
    "    \n",
    "    # number of parameters = 1 per input (features in original data) + bias = n_inputs + 1\n",
    "    # number of neurons in layer = n_hidden_neurons[i]\n",
    "    i = 0\n",
    "    for n in range(n_hidden):\n",
    "        if i == 0:\n",
    "            hidden_layer = [{'params': [np.random.rand() for n in range(n_inputs + 1)]} for n in range(n_hidden_neurons[i])]\n",
    "        else:\n",
    "            hidden_layer = [{'params': [np.random.rand() for n in range(n_hidden_neurons[i-1] + 1)]} for n in range(n_hidden_neurons[i])]\n",
    "        network.append(hidden_layer)\n",
    "        i += 1\n",
    "        \n",
    "    # number of parameters = 1 per input (neurons in previous hidden layer) + bias = n_hidden + 1\n",
    "    # number of neurons in layer = n_outputs\n",
    "    output_layer = [{'params': [np.random.rand() for n in range(n_hidden_neurons[i-1] + 1)]} for n in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network\n",
    "\n",
    "def print_layers(network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        if i < n_hidden:\n",
    "            print(f'HIDDEN LAYER {i+1}')\n",
    "            print(layer)\n",
    "            print(' ')\n",
    "        if i == n_hidden:\n",
    "            print('OUTPUT LAYER')\n",
    "            print(layer)\n",
    "        i += 1\n",
    "        \n",
    "network = init_network(n_inputs, n_hidden, n_hidden_neurons, n_outputs)\n",
    "\n",
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5762d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(output):\n",
    "    return 1.0 / (1.0 + np.exp(-output))\n",
    "\n",
    "def ReLu(output):\n",
    "    return max(0, output)\n",
    "\n",
    "def tanh(output):\n",
    "    return (np.exp(output)-np.exp(-output))/(np.exp(output)+np.exp(-output))\n",
    "\n",
    "activation_functions = ('sigmoid', 'ReLu', 'tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303d0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8126336592515571, 0.7787473036856518]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Forward propagate\n",
    "\n",
    "# Calculates the output of a single neuron -> (weights * inputs) + bias\n",
    "def calc_neuron_output(params, inputs):\n",
    "    bias = params[-1]\n",
    "    output = bias\n",
    "    for i in range(len(params) - 1): # for every weight\n",
    "        output += params[i] * inputs[i]\n",
    "    return output\n",
    "\n",
    "def forward_propagate(network, inputs):\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        # this list will store the activated output of each neuron to be the input of the next layer\n",
    "        # (in case the current layer is a hidden layer). Otherwise, this list will represent the outputs of the model\n",
    "        next_inputs = []\n",
    "        \n",
    "        for neuron in layer:\n",
    "            neuron_out = calc_neuron_output(neuron['params'], inputs) # linear output of neuron\n",
    "            neuron['output_activated'] = sigmoid(neuron_out) # sigmoid activation of linear output\n",
    "            next_inputs.append(neuron['output_activated'])\n",
    "            \n",
    "        inputs = next_inputs\n",
    "\n",
    "    return inputs # outputs of output layer\n",
    "\n",
    "forward_propagate(network, train[0][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e095d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives of activation functions\n",
    "def d_sigmoid(s):\n",
    "    return s*(1-s)\n",
    "\n",
    "def d_ReLu(r):\n",
    "    return 1 if r > 0 else 0\n",
    "\n",
    "def d_tanh(t):\n",
    "    return 1-t**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c81b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.3250176978548759, 0.7263017282699172, 0.04944697288034661], 'output_activated': 0.9429888607270035}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.5172413807841587, 0.9794618021114261], 'output_activated': 0.8126336592515571}, {'params': [0.8021491353998991, 0.5019634378645447], 'output_activated': 0.7787473036856518}]\n"
     ]
    }
   ],
   "source": [
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Back propagate error\n",
    "\n",
    "# delta (error) for neuron in output layer = (y_pred-y_expected) * d_actv\n",
    "# delta (error) for neuron in hidden layers = sum(all connected weights from top layer * corresponding delta)\n",
    "\n",
    "# i is the ith layer of the network we are iterating through\n",
    "# expected_output are the expected outputs of the network (neurons in the output layer)\n",
    "##([1,0] for 0 , [0,1] for 1) (answer corresponds to the index where 1 is)\n",
    "\n",
    "def backpropagate(network, i, expected_output):\n",
    "    \n",
    "    # Base case -- backpropagation starts in output layer\n",
    "    if i == n_hidden:\n",
    "\n",
    "        for n in range(len(network[i])): # loop through each neuron in the layer i of the network (output layer)\n",
    "            neuron = network[i][n] # current neuron\n",
    "            error = neuron['output_activated'] - expected_output[n] # error for output in neuron n of output layer\n",
    "            neuron['delta'] = error * d_sigmoid(neuron['output_activated'])\n",
    "        return\n",
    "    # End of base case\n",
    "\n",
    "    backpropagate(network, i + 1, expected_output)\n",
    "    \n",
    "    for n in range(len(network[i])): # loop through each neuron in the layer i of the network (hidden layer)\n",
    "        neuron = network[i][n] # current neuron\n",
    "        error = 0.0\n",
    "        for top_neuron in network[i+1]: # for each neuron in layer above\n",
    "            # (weights of top layer that the neuron output was multiplied by) * (corresponding delta)\n",
    "            error += top_neuron['params'][n] * top_neuron['delta']\n",
    "        neuron['delta'] = error * d_sigmoid(neuron['output_activated'])\n",
    "        \n",
    "    return\n",
    "            \n",
    "backpropagate(network, 0, [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6417c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER 1\n",
      "[{'params': [0.3250176978548759, 0.7263017282699172, 0.04944697288034661], 'output_activated': 0.9429888607270035, 'delta': 0.00499302841719589}]\n",
      " \n",
      "OUTPUT LAYER\n",
      "[{'params': [0.5172413807841587, 0.9794618021114261], 'output_activated': 0.8126336592515571, 'delta': -0.028528435598089597}, {'params': [0.8021491353998991, 0.5019634378645447], 'output_activated': 0.7787473036856518, 'delta': 0.13417811423596138}]\n"
     ]
    }
   ],
   "source": [
    "print_layers(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbbab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train network\n",
    "\n",
    "def update_weights(network, row, lr):\n",
    "    for i in range(len(network)): # for every layer in the network\n",
    "        inputs = row[:-1] # take training inputs\n",
    "        if i != 0: # for all layers except the first\n",
    "            inputs = [neuron['output_activated'] for neuron in network[i-1]] # inputs are the output of the previous layer\n",
    "        for neuron in network[i]: # for every neuron in the layer\n",
    "            for j in range(len(inputs)): # for every input to the layer (every weight in the neuron)\n",
    "                neuron['params'][j] -= lr * neuron['delta'] * inputs[j] # weight update\n",
    "            neuron['params'][-1] -= lr * neuron['delta'] # bias update\n",
    "\n",
    "def train_network(network, training_data, lr, n_epochs):\n",
    "    \n",
    "    for n_epoch in range(n_epochs):\n",
    "        sse = 0.0\n",
    "        \n",
    "        for row in training_data:\n",
    "            output = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(len(output))] # initialize to an array of 0s of same size as outputs\n",
    "            expected[row[-1]] = 1 # if actual output is 1, expected is [0,1], if 0 it is [1,0]\n",
    "            sse += sum([(expected[i]-output[i])**2 for i in range(len(expected))])\n",
    "            backpropagate(network, 0, expected)\n",
    "            update_weights(network, row, lr)\n",
    "        \n",
    "        if n_epoch % 50 == 0:\n",
    "            print('>epoch=%d, error=%.3f' % (n_epoch, sse)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44af0b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, error=5.742\n",
      ">epoch=50, error=1.527\n",
      ">epoch=100, error=0.538\n",
      ">epoch=150, error=0.303\n",
      ">epoch=200, error=0.206\n",
      ">epoch=250, error=0.154\n",
      ">epoch=300, error=0.123\n",
      ">epoch=350, error=0.102\n",
      ">epoch=400, error=0.086\n",
      ">epoch=450, error=0.075\n",
      ">epoch=500, error=0.066\n"
     ]
    }
   ],
   "source": [
    "def backprop(training_data, lr, n_epochs, n_inputs, n_hidden, n_hidden_neurons, n_outputs):\n",
    "    model = init_network(n_inputs, n_hidden, n_hidden_neurons, n_outputs)\n",
    "    train_network(model, training_data, lr, n_epochs)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "lr = 0.2\n",
    "training_data = train\n",
    "n_epochs = 501\n",
    "\n",
    "model = backprop(training_data, lr, n_epochs, n_inputs, n_hidden, n_hidden_neurons, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897c88a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0\n",
      "Prediction:  0\n",
      " ---- \n",
      "Label:  0\n",
      "Prediction:  0\n",
      " ---- \n",
      "Label:  0\n",
      "Prediction:  0\n",
      " ---- \n",
      "Label:  0\n",
      "Prediction:  0\n",
      " ---- \n",
      "Label:  0\n",
      "Prediction:  0\n",
      " ---- \n",
      "Label:  1\n",
      "Prediction:  1\n",
      " ---- \n",
      "Label:  1\n",
      "Prediction:  1\n",
      " ---- \n",
      "Label:  1\n",
      "Prediction:  1\n",
      " ---- \n",
      "Label:  1\n",
      "Prediction:  1\n",
      " ---- \n",
      "Label:  1\n",
      "Prediction:  1\n",
      " ---- \n"
     ]
    }
   ],
   "source": [
    "#### 5. Predict\n",
    "def predict(model, train):\n",
    "    \n",
    "    preds = [] # stores the predictions of all data points of the training data\n",
    "    \n",
    "    for row in training_data:\n",
    "        label = row[-1]\n",
    "        features = row[:-1]\n",
    "        outputs = forward_propagate(model, features)\n",
    "        pred = outputs.index(max(outputs))\n",
    "        preds.append(pred)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "preds = predict(model, train)\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    print('Label: ', training_data[i][-1])\n",
    "    print('Prediction: ', preds[i])\n",
    "    print(' ---- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ff529cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a csv file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95689dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, error=108.006\n",
      ">epoch=50, error=49.388\n",
      ">epoch=100, error=46.628\n",
      ">epoch=150, error=29.770\n",
      ">epoch=200, error=23.902\n",
      ">epoch=250, error=22.339\n",
      ">epoch=300, error=21.581\n",
      ">epoch=350, error=21.149\n",
      ">epoch=400, error=20.879\n",
      ">epoch=450, error=20.698\n",
      ">epoch=500, error=20.572\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict on fraud detection dataset - https://www.kaggle.com/datasets/whenamancodes/fraud-detection?resource=download\n",
    "\n",
    "ccard_data = load_csv('creditcard.csv')\n",
    "ccard_data_mod = []\n",
    "i = 0\n",
    "for row in ccard_data: # take only first 2 features and output, convert from string to float/integer\n",
    "    if i != 0:\n",
    "        new_row = row[1:3]\n",
    "        for j in range(len(new_row)):\n",
    "            new_row[j] = float(new_row[j])\n",
    "        new_row.append(int(row[30]))\n",
    "\n",
    "        ccard_data_mod.append(new_row)\n",
    "    i += 1\n",
    "\n",
    "traincc = ccard_data_mod[:8000]\n",
    "testcc = ccard_data_mod[8000:10000]\n",
    "\n",
    "lr = 0.1\n",
    "training_data = traincc\n",
    "n_epochscc = 501\n",
    "\n",
    "n_hiddencc = 1 # number of hidden layers\n",
    "n_inputscc = len(traincc[0][:-1]) # number of features\n",
    "n_hidden_neuronscc = [1,1] # number of neurons in hidden layer\n",
    "n_outputscc = 2 # number of possible outputs to be predicted\n",
    "\n",
    "modelcc = backprop(traincc, lr, n_epochscc, n_inputscc, n_hiddencc, n_hidden_neuronscc, n_outputscc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e285659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "class ConfusionMatrix:\n",
    "    \n",
    "    def __init__(self, modelcm, data):\n",
    "        self.model = modelcm\n",
    "        self.data = data\n",
    "        self.conf_matrix = None\n",
    "        \n",
    "        # METRICS\n",
    "        self.accuracy = 0.0\n",
    "        self.precision = 0.0\n",
    "        self.recall = 0.0\n",
    "        self.fprate = 0.0\n",
    "        self.fnrate = 0.0\n",
    "        \n",
    "    def predict(self):\n",
    "    \n",
    "        preds = [] # stores the predictions of all data points of the training data\n",
    "\n",
    "        for row in self.data:\n",
    "            features = row[:-1]\n",
    "            outputs = forward_propagate(self.model, features)\n",
    "            pred = outputs.index(max(outputs))\n",
    "            preds.append(pred)\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def print_matrix(self):\n",
    "        print(self.conf_matrix)\n",
    "        print(' ')\n",
    "        \n",
    "    def print_metrics(self):\n",
    "        # Accuracy (what fraction does it get right) = (# TP + # TN) / Total\n",
    "        self.accuracy = (self.conf_matrix['TPs'] + self.conf_matrix['TNs']) / sum(self.conf_matrix.values())\n",
    "        print('ACCURACY: ', round(self.accuracy, 3))\n",
    "        print(' ')\n",
    "        \n",
    "        # Precision (when it says 1, how often is it right) = # TP / (# TP + # FP)\n",
    "        self.precision = self.conf_matrix['TPs'] / (self.conf_matrix['TPs'] + self.conf_matrix['FPs'])\n",
    "        print('PRECISION: ', round(self.precision, 3))\n",
    "        print(' ')\n",
    "        \n",
    "        # Recall (what fraction of 1s does it get right) = # TP / (# TP + # FN)\n",
    "        self.recall = self.conf_matrix['TPs'] / (self.conf_matrix['TPs'] + self.conf_matrix['FNs'])\n",
    "        print('RECALL: ', round(self.recall, 3))\n",
    "        print(' ')\n",
    "        \n",
    "        # False positive rate (what fraction of 0s are called 1s) = # FP / (# FP + # TN)\n",
    "        self.fprate = self.conf_matrix['FPs'] / (self.conf_matrix['FPs'] + self.conf_matrix['TNs'])\n",
    "        print('FALSE POSITIVE RATE: ', round(self.fprate, 3))\n",
    "        print(' ')\n",
    "        \n",
    "        # False negative rate (what fraction of 1s are called 0s) = # FN / (# TP + # FN)\n",
    "        self.fnrate = self.conf_matrix['FNs'] / (self.conf_matrix['TPs'] + self.conf_matrix['FNs'])\n",
    "        print('FALSE NEGATIVE RATE: ', round(self.fnrate, 3))\n",
    "    \n",
    "    \n",
    "    def plot_matrix(self):\n",
    "        matrix_arr = [[self.conf_matrix['TNs'], self.conf_matrix['FPs']], [self.conf_matrix['FNs'], self.conf_matrix['TPs']]]\n",
    "        plt.imshow(matrix_arr, cmap = 'coolwarm', alpha = 0.5)\n",
    "        plt.xticks(np.arange(0, 2), ['0', '1'])\n",
    "        plt.yticks(np.arange(0, 2), ['0', '1'])\n",
    "        \n",
    "        plt.text(-0.1, 0, matrix_arr[0][0], fontsize = 14) # TNs\n",
    "        plt.text(0.95, 0, matrix_arr[0][1], fontsize = 14) # FPs\n",
    "        plt.text(-0.1, 1, matrix_arr[1][0], fontsize = 14) # FNs\n",
    "        plt.text(0.95, 1, matrix_arr[1][1], fontsize = 14) # TPs\n",
    "\n",
    "        plt.xlabel('Predictions', fontsize=18)\n",
    "        plt.ylabel('Actuals', fontsize=18)\n",
    "        plt.title('Confusion Matrix', fontsize=18)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        self.conf_matrix = {'TPs': 0, 'TNs': 0, 'FPs': 0, 'FNs': 0}\n",
    "        \n",
    "        preds = self.predict()\n",
    "        \n",
    "        for i in range(len(preds)):\n",
    "            label = self.data[i][-1]\n",
    "            pred = preds[i]\n",
    "            if label == 1 and pred == 1: # truly predicted positive\n",
    "                self.conf_matrix['TPs'] += 1\n",
    "            elif label == 0 and pred == 1: # falsely predicted positive\n",
    "                self.conf_matrix['FPs'] += 1\n",
    "            elif label == 1 and pred == 0: # falsely predicted negative\n",
    "                self.conf_matrix['FNs'] += 1\n",
    "            elif label == 0 and pred == 0: # truly predicted negative\n",
    "                self.conf_matrix['TNs'] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "739e5808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.996\n",
      " \n",
      "PRECISION:  0.625\n",
      " \n",
      "RECALL:  0.769\n",
      " \n",
      "FALSE POSITIVE RATE:  0.003\n",
      " \n",
      "FALSE NEGATIVE RATE:  0.231\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cc_cm = ConfusionMatrix(modelcc, testcc)\n",
    "cc_cm.fit()\n",
    "cc_cm.print_metrics()\n",
    "cc_cm.plot_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df9272f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Predict on titanic dataset - https://www.kaggle.com/competitions/titanic/data\n",
    "titanic_train = load_csv('titanic_train.csv')\n",
    "\n",
    "def modify_titanic(titanic_data, is_train = True):\n",
    "    # take max and min of variables to be normalized\n",
    "    age_min = 100\n",
    "    age_max = 0\n",
    "    sibsp_min = 100\n",
    "    sibsp_max = 0\n",
    "    parch_min = 100\n",
    "    parch_max = 0\n",
    "    fare_min = 100.0\n",
    "    fare_max = 0.0\n",
    "    \n",
    "    # Test set indexes\n",
    "    gender_ind = 3\n",
    "    age_ind = 4\n",
    "    sibsp_ind = 5\n",
    "    parch_ind = 6\n",
    "    fare_ind = 8\n",
    "    \n",
    "    # Adjust for train set (which includes 'Survived' label, hence the += 1)\n",
    "    if is_train:\n",
    "        gender_ind += 1\n",
    "        age_ind += 1\n",
    "        sibsp_ind += 1\n",
    "        parch_ind += 1\n",
    "        fare_ind += 1\n",
    "\n",
    "    j = 0\n",
    "    for row in titanic_data:\n",
    "        if j != 0:\n",
    "            age = int(row[age_ind]) if row[age_ind].isdigit() else age_min\n",
    "            if age < age_min:\n",
    "                age_min = age\n",
    "            if age > age_max:\n",
    "                age_max = age\n",
    "\n",
    "            sibsp = int(row[sibsp_ind]) if row[sibsp_ind].isdigit() else sibsp_min\n",
    "            if sibsp < sibsp_min:\n",
    "                sibsp_min = sibsp\n",
    "            if sibsp > sibsp_max:\n",
    "                sibsp_max = sibsp\n",
    "\n",
    "            parch = int(row[parch_ind]) if row[parch_ind].isdigit() else parch_min\n",
    "            if parch < parch_min:\n",
    "                parch_min = parch\n",
    "            if parch > parch_max:\n",
    "                parch_max = parch\n",
    "\n",
    "            fare = float(row[fare_ind]) if row[fare_ind].isdigit() else fare_min\n",
    "            if fare < fare_min:\n",
    "                fare_min = fare\n",
    "            if fare > fare_max:\n",
    "                fare_max = fare\n",
    "        j += 1\n",
    "    \n",
    "    titanic_mod = []\n",
    "    invalids = [0] # indexes of invalid rows\n",
    "\n",
    "    for i in range(len(titanic_data)): # take only first 2 features and output, convert from string to float/integer\n",
    "\n",
    "        if i != 0:\n",
    "\n",
    "            row = titanic_data[i]\n",
    "            new_row = []\n",
    "\n",
    "            gender = 1 if row[gender_ind] == 'male' else 0 # ind = 3 (test), 4 (train)\n",
    "            new_row.append(gender)\n",
    "\n",
    "            # scale 'Age' variable (row[5]) to [0,1] range and append to new_row\n",
    "            if row[age_ind].isdigit(): # ind = 4 (test), 5 (train)\n",
    "                age = int(row[age_ind])\n",
    "                age_scaled = (age - age_min) / (age_max - age_min)\n",
    "                new_row.append(age_scaled)\n",
    "\n",
    "            # scale 'SibSp' variable (row[6]) and append to new_row\n",
    "            if row[sibsp_ind].isdigit(): # ind = 5 (test), 6 (train)\n",
    "                sibsp = int(row[sibsp_ind])\n",
    "                sibsp_scaled = (sibsp - sibsp_min) / (sibsp_max - sibsp_min)\n",
    "                new_row.append(sibsp_scaled)\n",
    "\n",
    "            # scale 'Parch' variable (row[7]) and append to new_row\n",
    "            if row[parch_ind].isdigit(): # ind = 6 (test), 7 (train)\n",
    "                parch = int(row[parch_ind])\n",
    "                parch_scaled = (parch - parch_min) / (parch_max - parch_min)\n",
    "                new_row.append(parch_scaled)\n",
    "\n",
    "            # scale 'Fare' variable (row[9]) and append to new_row\n",
    "            if row[fare_ind].replace('.','',1).isdigit(): # ind = 8 (test), 9 (train)\n",
    "                fare = float(row[fare_ind])\n",
    "                fare_scaled = (fare - fare_min) / (fare_max - fare_min)\n",
    "                new_row.append(fare_scaled)\n",
    "\n",
    "            # append label ('Survived') (row[1]) to new_row\n",
    "            if is_train:\n",
    "                new_row.append(int(row[1]))\n",
    "\n",
    "            if row[age_ind].isdigit() and row[sibsp_ind].isdigit() and row[parch_ind].isdigit() and row[fare_ind].replace('.','',1).isdigit():\n",
    "                titanic_mod.append(new_row)\n",
    "            else:\n",
    "                invalids.append(i)\n",
    "                \n",
    "    return (titanic_mod, invalids)\n",
    "\n",
    "titanic_train_mod = modify_titanic(titanic_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adb20ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, error=341.353\n",
      ">epoch=50, error=213.101\n",
      ">epoch=100, error=206.570\n",
      ">epoch=150, error=201.766\n",
      ">epoch=200, error=199.638\n",
      ">epoch=250, error=198.131\n",
      ">epoch=300, error=196.711\n",
      ">epoch=350, error=195.456\n",
      ">epoch=400, error=194.406\n",
      ">epoch=450, error=193.523\n",
      ">epoch=500, error=192.747\n"
     ]
    }
   ],
   "source": [
    "lr = 0.2\n",
    "training_data = titanic_train_mod\n",
    "n_epochs_titanic = 501\n",
    "\n",
    "n_hidden_titanic = 1 # number of hidden layers\n",
    "n_inputs_titanic = len(titanic_train_mod[0][:-1]) # number of features\n",
    "n_hidden_neurons_titanic = [5,1] # number of neurons in hidden layer\n",
    "n_outputs_titanic = 2 # number of possible outputs to be predicted\n",
    "\n",
    "model_titanic = backprop(titanic_train_mod, lr, n_epochs_titanic, n_inputs_titanic, n_hidden_titanic, n_hidden_neurons_titanic, n_outputs_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74cc46c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY:  0.811\n",
      " \n",
      "PRECISION:  0.802\n",
      " \n",
      "RECALL:  0.716\n",
      " \n",
      "FALSE POSITIVE RATE:  0.123\n",
      " \n",
      "FALSE NEGATIVE RATE:  0.284\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAEiCAYAAADu9vesAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgMklEQVR4nO3debyUZf3/8dcHkEW2w46gcEAWATUQBTP9SeFabkC5lYW44FZpKmKmkmYKZZqJpvY1FBO0BFwyDVJSFKE0F9zIBXNBFlk87Nvn98d1H5wzzDnnGphh5hzfz8djHnPmvq/7uj9zZuY991z3PfeYuyMiUp06hS5ARGoGhYWIRFFYiEgUhYWIRFFYiEgUhYWIRFFY1HBm1tfM/mFmy83MzWxMntYzPOl/UD76r02S/9OEQteRawqL7WRmu5rZhWb2rJktM7ONZrbIzB5PXlj1dkIN9YCHgO7AlcBpwJR8r7dQzKw0eSG6mT1WSZtdzGxJ0mbBDqzrhHwFb01lOigre2bWDfgr0AOYAfwdWAq0BQ5LLr9y91F5rqMH8DZwsbv/Js/rqgvsAmxw9y35XFcVNZQC7wPrklr2cPeFaW2GAX9J2ixy99LtXNcE4AfubtuxbENgs7tv3J51F6u8v/vVNmbWCHgM6AoMc/f0d/KxZnYAcMBOKKd9cr0s3yty983A5nyvJ9KjwBDCltS4tHkjgFeBukCTnVVQ8rzY6O6b3H3dzlrvTuXuumRxAX4IOHBDlsudADwHrEouzwHHZ2i3AJgJ7EXYeikDVhLeLduntJuZ1JF+KQWGJ38PytD/TGBB2rSDgL8BnxLekT8GHgcOTGmTsU+gNTAe+BDYkFyPB1qltStf/hvAJcC7wHpgPuEdPOZ/WJr0cSvwMPBm2vzdgE3AhcC8DPdzADAhWeea5H/7HDAkw/8o0/92eDJ/QnK7DXA3sAjYApQm8x2YkNLfecm0K9PW0wFYArwJ7Fro53Z1F21ZZO/byfWdsQuY2XmEF9BbwC9InnjANDMb6e7pfXUkPGGnApcCXwFGAs2AI5I21xGe6D9Nank2mb4kmztjZj2B6YSg+C3hid8e+Fqy3heqWLY58DzQjfCieQnoB5wLfMPMBrh7WdpivwQaAXcQwuJcYIKZvePuz2VR+t2E/99X3X12Mu0HhK2f+4AzMywzhBDCDwIfAK2SZaaY2Xfd/f6k3XWE8bxDCFsv5Z5P66/8/3Yt0JjwJrANd7/NzAYDV5vZ0+4+y8zqJHU2BQ5z9zXxd71ACp1WNe0CfAZ8nkX7FoQn0TtAs5TpzQjvrmVAScr0BYQwOTGtn/HJ9L1Spg0i5R0vZfpwIrcsgB8lbQdUcz+26ZPwonLgvLS25yfTr82w/H+A+inTOxJCY1LE/7KUL7Ys6hFeqHemzH8L+Evyd6Yti8YZ+tyVMO7zRtr0CeHlkbGOCUkd91Uyv8KWRcrzYAHwv+TvK5N2FxT6OR170d6Q7DUDPs+i/eGEd51b3H3rcsnfvyN8rj4sbZlP3P3BtGlPJdfdsiu3WiuT6+OTgblsDCFsyaRvGd1BGPAdkmGZ29x9Q/kNd/+Y8LGgezYrdvdNwETgpGTP1NeAnoQtjsqWWV3+d7JMK0JYPAX0MrNm2dQA/DqLepcDpxI+Kv0NuBp4xN1vzXKdBaOwyN7nhE3HWF2S69czzJuXXHdNm/5ehrafJdetslh3jMmEPTo/BZaZ2VNmdpmZdY5YtgvwdvLC3Sq5/Tbb3i+o/L5tz/26mxDeQwkDm58AT1bW2MzamtmdZrYIWE0ItCXAOUmTkizXPz+bxu7+PDAWGJisd0SW6ysohUX25gHNzCzTCyGTrHe9UfVeh5j+qtofXmGcyt3Xu/vhhCfw9cm6rwHeMrNMWwY7qrL7lvX/yd3fBOYQPvacCNzrYa/Ntp2bGWEX9w+Ae4GTgKMIW37lYxVZvR48y3EGM6sPHJncbAl0ymb5QlNYZO+h5DrTAFom7ybXfTLM651cZ3q33RHlu1JbZpjXJcM03H2uu1+bBEc3wjvvL6pZz3tAz/QD0JLbPcj9/crkbuBAwse5P1bRbl/CgO0N7n6puz/o7k+6+wzCbtZ0+TgA6Xpgf2AUYQt1spk1zsN68kJhkb0/EDaxLzGz4zM1MLP+yR4QCCPmq4EfmlnTlDZNCbthVyVtcql887jCWIiZnULYXZc6rXWG5T8ibCZnCptU0wi7D9OD86xk+tS4cnfIZODnwI/dvaqPBeVbHBW2YMxsbzKPraxK5lf3P4hiZkcDFwH3uPuvCAO+PQiDtTWCdp1myd3XmNkxhGMgppnZ3wkv9s8IL5CvEzY1xyXtV5jZKMLejDkp3xkYTngHH+nuK8khd3/bzGYAI5PN75eBvoQXxTuEox/L/czMjiAcaPY+4cV0LGEXY/oBT+nGAd8BxpvZfoQ9Hf2AMwiBWt3yOywZKB4T0fRNwrjRKDMr3wPSg7BLeh6wX1r7F4ALgNvM7K/ARmCOu7+fbY1mthtwD/DfpE/c/a9m9lvgx2b2pLtPzrbfna7Qu2Nq6oUwin4RMAtYTngyLSKEyGlA3bT2Qwj76Vcnl+eBEzL0uwCYmWH6INJ2k2aaljKvPfBnwubuKsIIfC+23XU6CHggWe9awkeYOYStBUtpN5zMB2W1AW4jbI1sTK7HA63T2mVcPplXoaYq/uelSR+3RrTNtOu0c/I/WUI4KGtu8riMSfotTWlbh7C34yPCVsk2B2VVse6tu06TfmYQDnbrl9auPuHYlJVAl0I/p6u76LshIhJFYxYiEkVhISJRFBYiEkVhISJRatSu0xaNm3jHkpzs9padZFWDkkKXIFn48H/z12zetC7jgWI1Kiw6lrRkysi8nnxKcmxW92MKXYJk4dwRA1ZUNk8fQ0QkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKIoLEQkisJCRKLUK3QBtdmf5jzD5Bef4+MVywDo3qY95x56JIN67A3A6KkTmfry3ArLfGX3Uh4862IAPlr+GYNvHpOx70sPP54zDz4sf8XLVg//5WYemfLbCtOaNW/NTbf/CwB355GHfss/n5rEmtUr6dqtL989/Ro67t6jEOXmjcIij9o1L+GSw4+jtGVbtrgz7eU5nD/pLh4aOYq92ncE4KCuPRk39Ptbl9mlbt2tf+/WvAWzLrmuQp/T33yFax7/M0f26btT7oME7XfryqgrJ2+9XafOFxvlf3v0Dp58/A+MOOdXtN+tK49OuYUbf3ka1934Dxo1alKIcvNCYZFHh+21b4XbFx12LJP+PYuXP3x/a1jUr1ePNk2bZVy+bp0628yb/uYrHNS1J3u0aJ2foiWjOnXr0bykzTbT3Z0ZT9zNN487h/0HHA3AGefeyIXn7M+c5x9h0OBTd3apeaOw2Ek2b9nCE6//hzUb1tOvU9et01/833t8ddzlNGvYiAM6d+OiwcfSqknTjH18uHwps9+fz83fOX1nlS2JpYv/x8XnH0i9ervQZc++DDvpUtq068TSxR+ycsUS+uxzyNa29es3pMdeA3h3/osKC4n39qJPOPkPN7J+0yZ2rd+AW08+k57tOgBwSLfeHN6rL7u3aMXHKz7j5n88xg/u+R1TRl5K/Xq7bNPXn198nha7NmZw2haL5FfXbn3DR4wOe1K28jMem3YrvxwzjGvH/Z2VK5cAYQwjVbPmrVmx7NNClJs3Cos869KqLdPOGc3n69by9zde5rKp9zFx+I/o0a4D39qn/9Z2Pdt1oM9unfjGTVcxc/7rHNG7b4V+Nm3ezJSX5zCk78AK4xqSf/v0HVThdtfu/Rh94aE898xD7Nm9HwBmVnEh922n1XAF3XVqZkeZ2dtm9o6ZjS5kLflSv149Ordqwz4dO3Hx4cfRq31HJsx+OmPbds2a065ZCQs+W7LNvKfnz2NJ2ed8Z7+D8l2yVKNhw8Z02L07iz9dQPPmYRxj5YqKj9nnn3+2zdZGTVewsDCzusB44GigN3CKmfUuVD07yxZ3NmzelHHestWrWFy2krYZBjwffPF5BpR2o0vrtvkuUaqxccN6Pv3kXZq3aEvrtnvQvKQNb7w2q8L8/779L/bs0b+KXmqeQn4MGQC84+7vAZjZZOB44I0C1pRTv57+MIN69KF9sxas3rCex179N3MXvMMd3x3J6vXruXXm4xzRuy9tmjTj4xXL+M2MR2jZuCmH9fpKhX4+WbGMWe+8ydihpxXonny5PfCn6+i732BatupI2edLeXTqraxfv5aDDhmKmXHYUSP468Pjad9hT9rt1oXHpt5Kgwa7MvCg4wpdek4VMiw6Ah+m3P4IGJjeyMzOBs4G6NC8xc6pLEeWrvqcSx+6lyWrymjasCE923Xgru+dyyHderFu4wbmL/qEaa/MpWzdWto0acbALt25+cQRNGnQsEI/f3lpNk0bNuLIXn0Lc0e+5JZ/9il3/O7HrCpbTtNmLenarR9X/HwKrdvsDsDRx45k44Z1/GnCVaxevZKue/blJ5ffW6uOsQAwdy/Mis2+Axzp7mcmt08DBrj7DytbZu+OnXzKyFE7q0TJgVndjyl0CZKFc0cM+GT92sUdM80r5ADnR8AeKbd3Bz4pUC0iUo1ChsW/gO5m1sXM6gMnA48UsB4RqULBxizcfZOZXQA8CdQF7nb31wtVj4hUraAHZbn748DjhaxBROLofBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEkVhISJRFBYiEiU6LMxsgJmdlTbteDN7zcw+NrNf5r48ESkW2WxZXA1s/T02M+sETALaAyuBy8zs9NyWJyLFIpuw+ArwXMrtkwED+rp7b+DvJD8zKCK1TzZh0Qr4NOX2kcAz7v5xcvsRoHuuChOR4pJNWKwA2gGYWQPgQOCZlPkONMpZZSJSVLL5kaGXgTPNbAYwBGhI+DWxcl2ARbkrTUSKSTZhcS1hXGIuYaxiurv/O2X+McCcHNYmIkUkOizc/Xkz248wVrESmFw+z8xaEYJkas4rFJGikNVvnbr7fGB+humfARflqigRKT46glNEolS6ZWFmT21Hf+7ug3egHhEpUlV9DOlK2B0qIlJ5WLh76U6sQ0SKnMYsRCSKwkJEomS169TMWgBnAAOBFmwbNhrgFKmlosPCzDoTvnXagXBQVjNgGV+ExlJgdR5qFJEikM3HkF8AJcBgwrdLDTiJEBrXA2XAITmuT0SKRDZhMRi4y92f5otdqubua9z9CuA1YGyuCxSR4pDt+SzmJX9vTK5Tv5I+HTg8F0WJSPHJJiyWAC2Tv8uAdUBpyvz66HwWIrVWNmHxOuHUeri7E76qfp6ZdTKzUsIp9d7KeYUiUhSy2XX6MHCxmTVy97XANYST37yfzHdgaI7rE5Eikc35LG4Dbku5/ZSZfRU4FdgMTHX353NfoogUg6wOykqXnCnr39U2FJEaT4d7i0iUbI7gvDuimbv7GTtQj4gUqWw+hgyPaOOE746ISC0T/THE3eukX4BdgJ7AXcALhO+JiEgttKMDnJuB/wIjzexRwuHe5+aisEzWNm7Fq189LV/dSx6c0H9LoUuQLFxRUrfSebkc4PwbMCyH/YlIEcllWLQCmuSwPxEpIjv0MQTAzEqAwwi/G/LijvYnIsUpm12nW6j8bN9GOBHOT3JRlIgUn2y2LO5l27BwQkjMBya5e1muChOR4pLNd0OG57EOESly0QOcZnaVme1dxfw+ZnZVbsoSkWKTzd6QMcC+VczfG7h6h6oRkaKVy12nDYFNOexPRIpIlWMWZtaMcEbvcq3MrFOGpi2B7wIf5q40ESkm1Q1wXgSUj0M4cHNyycSAUTmpSkSKTnVhMTO5NkJoTAVeTWvjwCrgBZ0pS6T2qjIs3P2fwD9h6y+S/d7d5+yMwkSkuGRznMXp+SxERIpbNsdZnG9mM6qY/3czG5mbskSk2GSz63Q44dwVlZkPjNihakSkaGUTFt0Jv2damdeTNiJSC2UTFrsQDryqTMNq5otIDZZNWMyn6h8+PgJ4d8fKEZFilU1YTAKOMLNrzax++UQz28XMfk4Ii/tzXaCIFIdszmdxE3A0cAVwrpm9RTggqxfhcO9ngRtzXqGIFIVsfgpgI2HrYTTwEdAP2I/wfZBRwGDCkZ4iUgtl9a1Td9/o7uPcva+7N04u/YCngVuAT/JSpYgU3HafsNfMWgLfI/wC2d6ErYr5OapLRIpM1uezMLMjzewB4GPCOEZ94OfAPu6+V47rE5EiEbVlYWZdgNOBHwC7A0uAvwCnAle4+5S8VSgiRaHKLQszO9XM/kE4zHsU8G9gCNCRsDWhAU2RL4nqtizuA94DLgTud/dl5TPMrLLfEBGRWqi6MYsNQClwPHC0mTXKe0UiUpSqC4v2hK2KVsBEYJGZ/Z+Z/T/0EUTkS6XKsHD3Fe5+q7vvB+xPCIwTCMdVzCIcwdk830WKSOFlcwTnS+5+PtABOI3wlXSAP5jZy2b2MzPrk48iRaTwsj7Owt3Xu/v97j4Y2BO4DmgBXAO8kuP6RKRI7NCPDLn7Ane/ijAI+k1Ax1uI1FLbfbh3Knd34InkIiK1UC5/vlBEajGFhYhEUViISBSFhYhEUViISBSFhYhEUViISBSFhYhEUViISBSFRR5t3ryZyRN+wXmn7cup32rHeafty6Q//oLNmzdtbePuPHjv9Zx98l6cekx7rr7kW3y44M0CVi3pbrrpJgYPHkznzp3o0aM7p556Cm+++UaFNu7O2LE30Lt3bzp27MBxxx3LW2998TguX76cyy67jIEDB9KxYwf22WdvLr74YpYtW5a+uqKlsMijhx+8mSce/QMjzhvLzf83l9PPu4EnH72LqZN+k9Lmtzz60HhGnD+WG373FM1L2nDt6CGsXVNWwMol1XPPzWLEiBE88cQTTJv2MHXr1mPo0KEsX758a5tbbrmF8eNv44YbbmDGjBm0bt2aoUOHUVYWHsdPP13IwoULGTNmDM8+O4vf//4OZs9+nrPOOrNQdytrFr7WUTPs2aOfjx0/s9BlRLv+ypNo2rQFF4z6/dZpt447h7Ky5Vx+7QO4O2efshdHHXcWw069BID169dy5ond+f5Z13L4MacXqvSc+Ub/LYUuIedWrVpFly6lTJx4H0cddRTuTp8+vTnjjDO5+OKLAVi7di09e/bkmmuuYfjw4Rn7mT59OqeccjLvvfc+zZo124n3oHJ9+vT+ZOHChR0zzdOWRR7t1edA5r0yi4//F35O5cMP3mLey8+y3wHh96UXf/oBK5Yt4iv9v7F1mQYNGtFrn4N4+405BalZqrdq1Sq2bNlCSUkJAB988AGLFi3i61//+tY2jRo14qCDvsrcuXMr7aes7HMaNGjArrvumu+ScyIn3zqVzE446ULWrl3FRWcNpE6dumzevImhp1zCkceFTc8VyxYB0LxFmwrLlbRow7KlC3d6vRLnpz+9nH322YcDDjgAgMWLw+PYtm3bCu3atGnDwoWZH8eVK1dy/fXXc9pp36devZrxMqwZVdZQz8+cwjPTJ/Pj0X9g99K9WPDua/zx9tG0bd+JwUd/f2s7SzudqbuD6RSnxehnP7uCF16Yw+OPP07dunUrzEt/yNwdy/A4rl69mlNPPYXddtuNMWPG5LHa3CrYxxAzu9vMFpvZvELVkG8T77qKY79zAV/7+jA6d+nDoYedzLFDz2fqAzcBUNKyHQArli+usNzKFUspKWmzTX9SWFdc8VOmTJnCtGnTKC0t3Tq9bdvwOC5aVPFxXLp0KW3aVNzaWLVqFSeddCIA998/iYYNG+a36Bwq5JjFBOCoAq4/79avX0OdOhXfferUqYtvCYN+bdt3pqRlO1596emt8zdsWMdb82bTs/fAnVqrVO3yy0fz0EMPMXXqNHr06FFhXufOnWnXrh0zZ87cOm3dunXMnj2bAQMGbJ1WVlbGiSd+J+xSn/wATZo02Vnl50TBPoa4+zNmVlqo9e8M/Q88imkP3Ezb9p3Zo/NevP/Oqzw6ZTyHHnYyAGbGt4acy5RJN9Jxj+7s1rEbD93/axo2bMzB3/h2gauXcpdeeikPPvgAEyfeR0lJCYsWhTGKxo0b06RJE8yMkSPP4Te/uZHu3bvTrdue3HjjjTRu3IRhw4YBISi+/e2wK3XixPtYs2YNa9asAaBFixbUr1+/YPcvVkF3nSZh8Zi7711Fm7OBswFat92j/+33vbaTqttxa9eUMfme65j73GOsXLGUFi3b8bVBw/j290ZRv37Y/HR3/jzxBqY/PoHVZSvotld/zrzg13Tq0rvA1edGbdh12qpVy4zTR40axWWXjQbC4zhu3FjuueceVqxYQf/+/Rk3bhy9eoXHcdasWRx//HEZ+3n44Uc4+OCD81N8lqradVr0YZGqph1nIbUjLL5MdJyFiOwwhYWIRCnkrtNJwGygp5l9ZGZnFKoWEaleIfeGnFKodYtI9vQxRESiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSiKCxEJIrCQkSimLsXuoZoZrYE+KDQdeRBa2BpoYuQrNTWx6yzu7fJNKNGhUVtZWb/dvf9C12HxPsyPmb6GCIiURQWIhJFYVEc7ix0AZK1L91jpjELEYmiLQsRiaKwEJEoCosCMrOjzOxtM3vHzEYXuh6pnpndbWaLzWxeoWvZ2RQWBWJmdYHxwNFAb+AUM+td2KokwgTgqEIXUQgKi8IZALzj7u+5+wZgMnB8gWuSarj7M8CyQtdRCAqLwukIfJhy+6NkmkhRUlgUjmWYpv3YUrQUFoXzEbBHyu3dgU8KVItItRQWhfMvoLuZdTGz+sDJwCMFrkmkUgqLAnH3TcAFwJPAm8CD7v56YauS6pjZJGA20NPMPjKzMwpd086iw71FJIq2LEQkisJCRKIoLEQkisJCRKIoLEQkisJCophZqZm5mY2palq+1iWFp7AocmY2KHnhpF5WmdmLZvbj5NurNU4SCGPMrG+ha5E49QpdgESbBDxO+E5JB2A4cDPQBzi7QDV9ADQCNm3HsqXA1cAC4OUc9it5orCoOV5y9/vKb5jZ7YQjP880syvdfVH6AmbW1N3L8lWQhyP61tWUfmXH6GNIDeXunxMOOzagq5ktMLOZZtbPzJ40s5XAq+Xtzay7mU00s4VmtiFp/ysza5zet5kdbGbPmdlaM1tkZrcCTTK0q3RswcyGmdnTZrbCzNYkZwS7xczqm9lw4Omk6R9TPl7NrKpfM6tnZpeZ2Rtmts7MPjOzqWa2T2V1mdkxZvavpP3C5D7XS2vfx8z+bGYfm9l6M/s0qf1bEQ/Fl4a2LGooMzOgW3Kz/Gf0OgFPAX8GHiJ5gZtZ/2T6CuAO4GPgK8CPgK+Z2aHuvjFpOxCYAZQBY5NlTgbuzaK264CfAm8ANwELgT2BYcBVwDPAL5M2dwLPJotus3WU5k/AicB04HagPXA+MNvMDnH3/6S1/yZwHvB74G7CyYUuAZYn68fMWhH+NyTtPiD8NOH+wEDgr7H3u9Zzd12K+AIMIpzn4irCk7gNsC9wVzJ9dtJuQXL7zAx9vAK8BTRNmz4kWWZ4yrTngQ1Aj5Rp9YG5SdsxKdNLM0wbkEx7CmiYtj7ji+8jDUpfdzX9Hp5Me6C8j2T6voSxjWczLL8aKE1b/zxgYcq045K2Jxb6sS72iz6G1Bw/B5YAiwkv/hGEr7SfkNJmGfDH1IWSTfR9gfuBBmbWuvwCzCK8oI5I2rYFvgo87O7zy/vwcNq/myLr/G5yfbm7Vxh38ERkP+mGJNfXpfbh7q8CjwEHm1n6D/pOc/cFqesnfPxpb2blH6tWJtdHm1mz7aztS0FhUXPcSXh3PYzwgm7j7sd7xYHNd919c9pyvZLr8rBJvSwGGgPtkjZdk+u3Mqz/jcg6uxPeqV+JbB+rC7CFMKibbl5Km1TvZWj7WXLdCsDd/0n4iDUcWJqM1fxcJ0/elsYsao7/uvuMatqsyTCt/PR9NwJPVLLc8rS2md79M50GMBOrZPkdFbv+VOnBmbE/d/+Bmf2KMMZxMHAxcIWZXejut27HemslhUXt99/kenNE2LybXPfKMC/TtEzeJpwqf1/COEdlsg2Ud4EjkzpeTZtXvhXwfpZ9flGM+zzCFso4MysB5gA3mNn4HfjoVKvoY0jt9x/Ci+AcM+uaPjPZHdkSwN0XAy8Ax5tZj5Q29YGLItd3f3L9SzNrkGF95e/oq5LrlpH9TkuuL0/pAzPbmzBIOcvdl0T2lVpPSzOr8Dpw9xWE4NkVaJhtn7WVtixqOXd3MzuNsHfiVTO7G3id8ELoBgwFLif8eA7AT4CZwHNmNp4vdp1GPVfcfa6ZjQUuA140sweATwnjCd8m7C1ZQRgDKQPOM7M1ybTF7v5UJf1ON7MHk1pamNljfLHrdB1hN/D2+D5wkZlNBd4BNgKHErZiHnT3tdvZb62jsPgScPeXzawfIRSOA84hvFAXEELiHyltZ5vZ4cANwGjgc8JxG7cDr0Wub7SZvUI4x+gowhbsh4TD1dckbdaa2cnALwiHrTcA/skXxzxk8l3gJcJg5I2EPTn/BK5096jaMpgJ9AOOAXYjjHO8TzgeQ+MVKXQOThGJojELEYmisBCRKAoLEYmisBCRKAoLEYmisBCRKAoLEYmisBCRKAoLEYny/wHpNLxA01ASOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf_matrix_titanic = ConfusionMatrix(model_titanic, titanic_train_mod)\n",
    "conf_matrix_titanic.fit()\n",
    "conf_matrix_titanic.print_metrics()\n",
    "conf_matrix_titanic.plot_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "440b91b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 11,\n",
       " 23,\n",
       " 28,\n",
       " 30,\n",
       " 34,\n",
       " 36,\n",
       " 37,\n",
       " 40,\n",
       " 42,\n",
       " 48,\n",
       " 55,\n",
       " 59,\n",
       " 66,\n",
       " 74,\n",
       " 76,\n",
       " 77,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 89,\n",
       " 92,\n",
       " 94,\n",
       " 103,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 112,\n",
       " 114,\n",
       " 117,\n",
       " 122,\n",
       " 125,\n",
       " 128,\n",
       " 133,\n",
       " 134,\n",
       " 137,\n",
       " 147,\n",
       " 149,\n",
       " 152,\n",
       " 153,\n",
       " 161,\n",
       " 164,\n",
       " 169,\n",
       " 171,\n",
       " 174,\n",
       " 184,\n",
       " 189,\n",
       " 192,\n",
       " 193,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 206,\n",
       " 212,\n",
       " 217,\n",
       " 220,\n",
       " 226,\n",
       " 228,\n",
       " 229,\n",
       " 234,\n",
       " 244,\n",
       " 245,\n",
       " 250,\n",
       " 251,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 261,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 272,\n",
       " 274,\n",
       " 275,\n",
       " 282,\n",
       " 283,\n",
       " 287,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 293,\n",
       " 298,\n",
       " 302,\n",
       " 305,\n",
       " 308,\n",
       " 313,\n",
       " 333,\n",
       " 340,\n",
       " 343,\n",
       " 345,\n",
       " 355,\n",
       " 358,\n",
       " 359,\n",
       " 361,\n",
       " 366,\n",
       " 367,\n",
       " 381,\n",
       " 383,\n",
       " 385,\n",
       " 409,\n",
       " 411,\n",
       " 414,\n",
       " 416,\n",
       " 417,\n",
       " 418]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaggle submission - .csv file with columns 'PassengerId', 'Survived'\n",
    "titanic_test = load_csv('titanic_test.csv')\n",
    "titanic_test_mod, test_invalids = modify_titanic(titanic_test, is_train = False)\n",
    "\n",
    "test_invalids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fcff342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_submission(model, data, data_mod, invalids):\n",
    "    \n",
    "    submission = []\n",
    "    mod_index = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        if i != 0:\n",
    "            if i in invalids:\n",
    "                pred = random.choice([0,1])\n",
    "            else:\n",
    "                features = data_mod[mod_index]\n",
    "                outputs = forward_propagate(model, features)\n",
    "                pred = outputs.index(max(outputs))\n",
    "                mod_index += 1\n",
    "            submission.append([i+891, pred])\n",
    "        \n",
    "    return submission\n",
    "\n",
    "# if index of row in titanic_test in invalids, predict randomly 0 or 1\n",
    "titanic_submission = predict_submission(model_titanic, titanic_test, titanic_test_mod, test_invalids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46e6708f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[892, 0],\n",
       " [893, 0],\n",
       " [894, 0],\n",
       " [895, 0],\n",
       " [896, 0],\n",
       " [897, 0],\n",
       " [898, 1],\n",
       " [899, 0],\n",
       " [900, 1],\n",
       " [901, 0],\n",
       " [902, 1],\n",
       " [903, 0],\n",
       " [904, 1],\n",
       " [905, 0],\n",
       " [906, 1],\n",
       " [907, 1],\n",
       " [908, 0],\n",
       " [909, 0],\n",
       " [910, 0],\n",
       " [911, 1],\n",
       " [912, 0],\n",
       " [913, 0],\n",
       " [914, 0],\n",
       " [915, 1],\n",
       " [916, 1],\n",
       " [917, 0],\n",
       " [918, 1],\n",
       " [919, 1],\n",
       " [920, 0],\n",
       " [921, 0],\n",
       " [922, 0],\n",
       " [923, 0],\n",
       " [924, 1],\n",
       " [925, 1],\n",
       " [926, 1],\n",
       " [927, 1],\n",
       " [928, 0],\n",
       " [929, 1],\n",
       " [930, 0],\n",
       " [931, 0],\n",
       " [932, 0],\n",
       " [933, 1],\n",
       " [934, 0],\n",
       " [935, 1],\n",
       " [936, 1],\n",
       " [937, 0],\n",
       " [938, 0],\n",
       " [939, 1],\n",
       " [940, 1],\n",
       " [941, 1],\n",
       " [942, 1],\n",
       " [943, 0],\n",
       " [944, 0],\n",
       " [945, 1],\n",
       " [946, 0],\n",
       " [947, 0],\n",
       " [948, 0],\n",
       " [949, 0],\n",
       " [950, 0],\n",
       " [951, 1],\n",
       " [952, 0],\n",
       " [953, 0],\n",
       " [954, 0],\n",
       " [955, 1],\n",
       " [956, 0],\n",
       " [957, 0],\n",
       " [958, 1],\n",
       " [959, 0],\n",
       " [960, 0],\n",
       " [961, 1],\n",
       " [962, 1],\n",
       " [963, 0],\n",
       " [964, 1],\n",
       " [965, 1],\n",
       " [966, 1],\n",
       " [967, 0],\n",
       " [968, 1],\n",
       " [969, 0],\n",
       " [970, 0],\n",
       " [971, 1],\n",
       " [972, 0],\n",
       " [973, 0],\n",
       " [974, 0],\n",
       " [975, 1],\n",
       " [976, 1],\n",
       " [977, 0],\n",
       " [978, 1],\n",
       " [979, 1],\n",
       " [980, 0],\n",
       " [981, 0],\n",
       " [982, 1],\n",
       " [983, 1],\n",
       " [984, 1],\n",
       " [985, 1],\n",
       " [986, 0],\n",
       " [987, 0],\n",
       " [988, 1],\n",
       " [989, 0],\n",
       " [990, 1],\n",
       " [991, 0],\n",
       " [992, 1],\n",
       " [993, 0],\n",
       " [994, 0],\n",
       " [995, 0],\n",
       " [996, 0],\n",
       " [997, 0],\n",
       " [998, 0],\n",
       " [999, 0],\n",
       " [1000, 0],\n",
       " [1001, 1],\n",
       " [1002, 0],\n",
       " [1003, 1],\n",
       " [1004, 1],\n",
       " [1005, 0],\n",
       " [1006, 1],\n",
       " [1007, 0],\n",
       " [1008, 0],\n",
       " [1009, 1],\n",
       " [1010, 1],\n",
       " [1011, 1],\n",
       " [1012, 1],\n",
       " [1013, 1],\n",
       " [1014, 1],\n",
       " [1015, 0],\n",
       " [1016, 0],\n",
       " [1017, 1],\n",
       " [1018, 0],\n",
       " [1019, 1],\n",
       " [1020, 0],\n",
       " [1021, 0],\n",
       " [1022, 0],\n",
       " [1023, 0],\n",
       " [1024, 0],\n",
       " [1025, 1],\n",
       " [1026, 0],\n",
       " [1027, 0],\n",
       " [1028, 1],\n",
       " [1029, 0],\n",
       " [1030, 1],\n",
       " [1031, 0],\n",
       " [1032, 0],\n",
       " [1033, 1],\n",
       " [1034, 0],\n",
       " [1035, 0],\n",
       " [1036, 0],\n",
       " [1037, 0],\n",
       " [1038, 0],\n",
       " [1039, 0],\n",
       " [1040, 1],\n",
       " [1041, 0],\n",
       " [1042, 1],\n",
       " [1043, 1],\n",
       " [1044, 0],\n",
       " [1045, 1],\n",
       " [1046, 0],\n",
       " [1047, 0],\n",
       " [1048, 1],\n",
       " [1049, 1],\n",
       " [1050, 0],\n",
       " [1051, 1],\n",
       " [1052, 1],\n",
       " [1053, 0],\n",
       " [1054, 1],\n",
       " [1055, 0],\n",
       " [1056, 0],\n",
       " [1057, 1],\n",
       " [1058, 0],\n",
       " [1059, 0],\n",
       " [1060, 0],\n",
       " [1061, 1],\n",
       " [1062, 1],\n",
       " [1063, 0],\n",
       " [1064, 0],\n",
       " [1065, 1],\n",
       " [1066, 0],\n",
       " [1067, 1],\n",
       " [1068, 1],\n",
       " [1069, 0],\n",
       " [1070, 1],\n",
       " [1071, 1],\n",
       " [1072, 0],\n",
       " [1073, 0],\n",
       " [1074, 1],\n",
       " [1075, 1],\n",
       " [1076, 1],\n",
       " [1077, 0],\n",
       " [1078, 1],\n",
       " [1079, 0],\n",
       " [1080, 0],\n",
       " [1081, 0],\n",
       " [1082, 0],\n",
       " [1083, 1],\n",
       " [1084, 1],\n",
       " [1085, 0],\n",
       " [1086, 1],\n",
       " [1087, 0],\n",
       " [1088, 0],\n",
       " [1089, 1],\n",
       " [1090, 0],\n",
       " [1091, 0],\n",
       " [1092, 0],\n",
       " [1093, 0],\n",
       " [1094, 0],\n",
       " [1095, 1],\n",
       " [1096, 0],\n",
       " [1097, 1],\n",
       " [1098, 1],\n",
       " [1099, 0],\n",
       " [1100, 1],\n",
       " [1101, 0],\n",
       " [1102, 0],\n",
       " [1103, 0],\n",
       " [1104, 1],\n",
       " [1105, 1],\n",
       " [1106, 0],\n",
       " [1107, 0],\n",
       " [1108, 0],\n",
       " [1109, 0],\n",
       " [1110, 1],\n",
       " [1111, 0],\n",
       " [1112, 1],\n",
       " [1113, 0],\n",
       " [1114, 1],\n",
       " [1115, 0],\n",
       " [1116, 1],\n",
       " [1117, 0],\n",
       " [1118, 0],\n",
       " [1119, 1],\n",
       " [1120, 0],\n",
       " [1121, 0],\n",
       " [1122, 1],\n",
       " [1123, 1],\n",
       " [1124, 0],\n",
       " [1125, 0],\n",
       " [1126, 0],\n",
       " [1127, 0],\n",
       " [1128, 0],\n",
       " [1129, 0],\n",
       " [1130, 1],\n",
       " [1131, 1],\n",
       " [1132, 1],\n",
       " [1133, 1],\n",
       " [1134, 0],\n",
       " [1135, 1],\n",
       " [1136, 0],\n",
       " [1137, 0],\n",
       " [1138, 1],\n",
       " [1139, 0],\n",
       " [1140, 1],\n",
       " [1141, 0],\n",
       " [1142, 1],\n",
       " [1143, 0],\n",
       " [1144, 0],\n",
       " [1145, 0],\n",
       " [1146, 0],\n",
       " [1147, 0],\n",
       " [1148, 1],\n",
       " [1149, 0],\n",
       " [1150, 1],\n",
       " [1151, 0],\n",
       " [1152, 1],\n",
       " [1153, 0],\n",
       " [1154, 1],\n",
       " [1155, 1],\n",
       " [1156, 0],\n",
       " [1157, 1],\n",
       " [1158, 0],\n",
       " [1159, 1],\n",
       " [1160, 0],\n",
       " [1161, 0],\n",
       " [1162, 1],\n",
       " [1163, 0],\n",
       " [1164, 1],\n",
       " [1165, 1],\n",
       " [1166, 1],\n",
       " [1167, 1],\n",
       " [1168, 0],\n",
       " [1169, 0],\n",
       " [1170, 0],\n",
       " [1171, 0],\n",
       " [1172, 1],\n",
       " [1173, 1],\n",
       " [1174, 1],\n",
       " [1175, 1],\n",
       " [1176, 1],\n",
       " [1177, 0],\n",
       " [1178, 0],\n",
       " [1179, 1],\n",
       " [1180, 1],\n",
       " [1181, 0],\n",
       " [1182, 1],\n",
       " [1183, 1],\n",
       " [1184, 1],\n",
       " [1185, 0],\n",
       " [1186, 0],\n",
       " [1187, 0],\n",
       " [1188, 1],\n",
       " [1189, 1],\n",
       " [1190, 1],\n",
       " [1191, 0],\n",
       " [1192, 0],\n",
       " [1193, 0],\n",
       " [1194, 0],\n",
       " [1195, 0],\n",
       " [1196, 0],\n",
       " [1197, 1],\n",
       " [1198, 0],\n",
       " [1199, 0],\n",
       " [1200, 0],\n",
       " [1201, 0],\n",
       " [1202, 0],\n",
       " [1203, 0],\n",
       " [1204, 0],\n",
       " [1205, 1],\n",
       " [1206, 1],\n",
       " [1207, 1],\n",
       " [1208, 0],\n",
       " [1209, 0],\n",
       " [1210, 0],\n",
       " [1211, 0],\n",
       " [1212, 0],\n",
       " [1213, 0],\n",
       " [1214, 0],\n",
       " [1215, 0],\n",
       " [1216, 1],\n",
       " [1217, 0],\n",
       " [1218, 1],\n",
       " [1219, 1],\n",
       " [1220, 0],\n",
       " [1221, 0],\n",
       " [1222, 1],\n",
       " [1223, 0],\n",
       " [1224, 1],\n",
       " [1225, 1],\n",
       " [1226, 0],\n",
       " [1227, 0],\n",
       " [1228, 0],\n",
       " [1229, 0],\n",
       " [1230, 0],\n",
       " [1231, 0],\n",
       " [1232, 0],\n",
       " [1233, 0],\n",
       " [1234, 1],\n",
       " [1235, 1],\n",
       " [1236, 0],\n",
       " [1237, 1],\n",
       " [1238, 0],\n",
       " [1239, 1],\n",
       " [1240, 0],\n",
       " [1241, 1],\n",
       " [1242, 1],\n",
       " [1243, 0],\n",
       " [1244, 1],\n",
       " [1245, 0],\n",
       " [1246, 1],\n",
       " [1247, 0],\n",
       " [1248, 1],\n",
       " [1249, 1],\n",
       " [1250, 0],\n",
       " [1251, 1],\n",
       " [1252, 0],\n",
       " [1253, 1],\n",
       " [1254, 1],\n",
       " [1255, 0],\n",
       " [1256, 1],\n",
       " [1257, 0],\n",
       " [1258, 1],\n",
       " [1259, 1],\n",
       " [1260, 1],\n",
       " [1261, 0],\n",
       " [1262, 0],\n",
       " [1263, 1],\n",
       " [1264, 0],\n",
       " [1265, 0],\n",
       " [1266, 1],\n",
       " [1267, 1],\n",
       " [1268, 0],\n",
       " [1269, 0],\n",
       " [1270, 0],\n",
       " [1271, 0],\n",
       " [1272, 1],\n",
       " [1273, 0],\n",
       " [1274, 0],\n",
       " [1275, 1],\n",
       " [1276, 1],\n",
       " [1277, 1],\n",
       " [1278, 0],\n",
       " [1279, 0],\n",
       " [1280, 0],\n",
       " [1281, 0],\n",
       " [1282, 1],\n",
       " [1283, 1],\n",
       " [1284, 1],\n",
       " [1285, 0],\n",
       " [1286, 0],\n",
       " [1287, 1],\n",
       " [1288, 0],\n",
       " [1289, 1],\n",
       " [1290, 0],\n",
       " [1291, 0],\n",
       " [1292, 1],\n",
       " [1293, 0],\n",
       " [1294, 1],\n",
       " [1295, 1],\n",
       " [1296, 0],\n",
       " [1297, 0],\n",
       " [1298, 0],\n",
       " [1299, 0],\n",
       " [1300, 0],\n",
       " [1301, 1],\n",
       " [1302, 0],\n",
       " [1303, 1],\n",
       " [1304, 1],\n",
       " [1305, 0],\n",
       " [1306, 1],\n",
       " [1307, 1],\n",
       " [1308, 1],\n",
       " [1309, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "199b7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of the csv file is:\n",
      "PassengerId,Survived\n",
      "892,0\n",
      "893,0\n",
      "894,0\n",
      "895,0\n",
      "896,0\n",
      "897,0\n",
      "898,1\n",
      "899,0\n",
      "900,1\n",
      "901,0\n",
      "902,1\n",
      "903,0\n",
      "904,1\n",
      "905,0\n",
      "906,1\n",
      "907,1\n",
      "908,0\n",
      "909,0\n",
      "910,0\n",
      "911,1\n",
      "912,0\n",
      "913,0\n",
      "914,0\n",
      "915,1\n",
      "916,1\n",
      "917,0\n",
      "918,1\n",
      "919,1\n",
      "920,0\n",
      "921,0\n",
      "922,0\n",
      "923,0\n",
      "924,1\n",
      "925,1\n",
      "926,1\n",
      "927,1\n",
      "928,0\n",
      "929,1\n",
      "930,0\n",
      "931,0\n",
      "932,0\n",
      "933,1\n",
      "934,0\n",
      "935,1\n",
      "936,1\n",
      "937,0\n",
      "938,0\n",
      "939,1\n",
      "940,1\n",
      "941,1\n",
      "942,1\n",
      "943,0\n",
      "944,0\n",
      "945,1\n",
      "946,0\n",
      "947,0\n",
      "948,0\n",
      "949,0\n",
      "950,0\n",
      "951,1\n",
      "952,0\n",
      "953,0\n",
      "954,0\n",
      "955,1\n",
      "956,0\n",
      "957,0\n",
      "958,1\n",
      "959,0\n",
      "960,0\n",
      "961,1\n",
      "962,1\n",
      "963,0\n",
      "964,1\n",
      "965,1\n",
      "966,1\n",
      "967,0\n",
      "968,1\n",
      "969,0\n",
      "970,0\n",
      "971,1\n",
      "972,0\n",
      "973,0\n",
      "974,0\n",
      "975,1\n",
      "976,1\n",
      "977,0\n",
      "978,1\n",
      "979,1\n",
      "980,0\n",
      "981,0\n",
      "982,1\n",
      "983,1\n",
      "984,1\n",
      "985,1\n",
      "986,0\n",
      "987,0\n",
      "988,1\n",
      "989,0\n",
      "990,1\n",
      "991,0\n",
      "992,1\n",
      "993,0\n",
      "994,0\n",
      "995,0\n",
      "996,0\n",
      "997,0\n",
      "998,0\n",
      "999,0\n",
      "1000,0\n",
      "1001,1\n",
      "1002,0\n",
      "1003,1\n",
      "1004,1\n",
      "1005,0\n",
      "1006,1\n",
      "1007,0\n",
      "1008,0\n",
      "1009,1\n",
      "1010,1\n",
      "1011,1\n",
      "1012,1\n",
      "1013,1\n",
      "1014,1\n",
      "1015,0\n",
      "1016,0\n",
      "1017,1\n",
      "1018,0\n",
      "1019,1\n",
      "1020,0\n",
      "1021,0\n",
      "1022,0\n",
      "1023,0\n",
      "1024,0\n",
      "1025,1\n",
      "1026,0\n",
      "1027,0\n",
      "1028,1\n",
      "1029,0\n",
      "1030,1\n",
      "1031,0\n",
      "1032,0\n",
      "1033,1\n",
      "1034,0\n",
      "1035,0\n",
      "1036,0\n",
      "1037,0\n",
      "1038,0\n",
      "1039,0\n",
      "1040,1\n",
      "1041,0\n",
      "1042,1\n",
      "1043,1\n",
      "1044,0\n",
      "1045,1\n",
      "1046,0\n",
      "1047,0\n",
      "1048,1\n",
      "1049,1\n",
      "1050,0\n",
      "1051,1\n",
      "1052,1\n",
      "1053,0\n",
      "1054,1\n",
      "1055,0\n",
      "1056,0\n",
      "1057,1\n",
      "1058,0\n",
      "1059,0\n",
      "1060,0\n",
      "1061,1\n",
      "1062,1\n",
      "1063,0\n",
      "1064,0\n",
      "1065,1\n",
      "1066,0\n",
      "1067,1\n",
      "1068,1\n",
      "1069,0\n",
      "1070,1\n",
      "1071,1\n",
      "1072,0\n",
      "1073,0\n",
      "1074,1\n",
      "1075,1\n",
      "1076,1\n",
      "1077,0\n",
      "1078,1\n",
      "1079,0\n",
      "1080,0\n",
      "1081,0\n",
      "1082,0\n",
      "1083,1\n",
      "1084,1\n",
      "1085,0\n",
      "1086,1\n",
      "1087,0\n",
      "1088,0\n",
      "1089,1\n",
      "1090,0\n",
      "1091,0\n",
      "1092,0\n",
      "1093,0\n",
      "1094,0\n",
      "1095,1\n",
      "1096,0\n",
      "1097,1\n",
      "1098,1\n",
      "1099,0\n",
      "1100,1\n",
      "1101,0\n",
      "1102,0\n",
      "1103,0\n",
      "1104,1\n",
      "1105,1\n",
      "1106,0\n",
      "1107,0\n",
      "1108,0\n",
      "1109,0\n",
      "1110,1\n",
      "1111,0\n",
      "1112,1\n",
      "1113,0\n",
      "1114,1\n",
      "1115,0\n",
      "1116,1\n",
      "1117,0\n",
      "1118,0\n",
      "1119,1\n",
      "1120,0\n",
      "1121,0\n",
      "1122,1\n",
      "1123,1\n",
      "1124,0\n",
      "1125,0\n",
      "1126,0\n",
      "1127,0\n",
      "1128,0\n",
      "1129,0\n",
      "1130,1\n",
      "1131,1\n",
      "1132,1\n",
      "1133,1\n",
      "1134,0\n",
      "1135,1\n",
      "1136,0\n",
      "1137,0\n",
      "1138,1\n",
      "1139,0\n",
      "1140,1\n",
      "1141,0\n",
      "1142,1\n",
      "1143,0\n",
      "1144,0\n",
      "1145,0\n",
      "1146,0\n",
      "1147,0\n",
      "1148,1\n",
      "1149,0\n",
      "1150,1\n",
      "1151,0\n",
      "1152,1\n",
      "1153,0\n",
      "1154,1\n",
      "1155,1\n",
      "1156,0\n",
      "1157,1\n",
      "1158,0\n",
      "1159,1\n",
      "1160,0\n",
      "1161,0\n",
      "1162,1\n",
      "1163,0\n",
      "1164,1\n",
      "1165,1\n",
      "1166,1\n",
      "1167,1\n",
      "1168,0\n",
      "1169,0\n",
      "1170,0\n",
      "1171,0\n",
      "1172,1\n",
      "1173,1\n",
      "1174,1\n",
      "1175,1\n",
      "1176,1\n",
      "1177,0\n",
      "1178,0\n",
      "1179,1\n",
      "1180,1\n",
      "1181,0\n",
      "1182,1\n",
      "1183,1\n",
      "1184,1\n",
      "1185,0\n",
      "1186,0\n",
      "1187,0\n",
      "1188,1\n",
      "1189,1\n",
      "1190,1\n",
      "1191,0\n",
      "1192,0\n",
      "1193,0\n",
      "1194,0\n",
      "1195,0\n",
      "1196,0\n",
      "1197,1\n",
      "1198,0\n",
      "1199,0\n",
      "1200,0\n",
      "1201,0\n",
      "1202,0\n",
      "1203,0\n",
      "1204,0\n",
      "1205,1\n",
      "1206,1\n",
      "1207,1\n",
      "1208,0\n",
      "1209,0\n",
      "1210,0\n",
      "1211,0\n",
      "1212,0\n",
      "1213,0\n",
      "1214,0\n",
      "1215,0\n",
      "1216,1\n",
      "1217,0\n",
      "1218,1\n",
      "1219,1\n",
      "1220,0\n",
      "1221,0\n",
      "1222,1\n",
      "1223,0\n",
      "1224,1\n",
      "1225,1\n",
      "1226,0\n",
      "1227,0\n",
      "1228,0\n",
      "1229,0\n",
      "1230,0\n",
      "1231,0\n",
      "1232,0\n",
      "1233,0\n",
      "1234,1\n",
      "1235,1\n",
      "1236,0\n",
      "1237,1\n",
      "1238,0\n",
      "1239,1\n",
      "1240,0\n",
      "1241,1\n",
      "1242,1\n",
      "1243,0\n",
      "1244,1\n",
      "1245,0\n",
      "1246,1\n",
      "1247,0\n",
      "1248,1\n",
      "1249,1\n",
      "1250,0\n",
      "1251,1\n",
      "1252,0\n",
      "1253,1\n",
      "1254,1\n",
      "1255,0\n",
      "1256,1\n",
      "1257,0\n",
      "1258,1\n",
      "1259,1\n",
      "1260,1\n",
      "1261,0\n",
      "1262,0\n",
      "1263,1\n",
      "1264,0\n",
      "1265,0\n",
      "1266,1\n",
      "1267,1\n",
      "1268,0\n",
      "1269,0\n",
      "1270,0\n",
      "1271,0\n",
      "1272,1\n",
      "1273,0\n",
      "1274,0\n",
      "1275,1\n",
      "1276,1\n",
      "1277,1\n",
      "1278,0\n",
      "1279,0\n",
      "1280,0\n",
      "1281,0\n",
      "1282,1\n",
      "1283,1\n",
      "1284,1\n",
      "1285,0\n",
      "1286,0\n",
      "1287,1\n",
      "1288,0\n",
      "1289,1\n",
      "1290,0\n",
      "1291,0\n",
      "1292,1\n",
      "1293,0\n",
      "1294,1\n",
      "1295,1\n",
      "1296,0\n",
      "1297,0\n",
      "1298,0\n",
      "1299,0\n",
      "1300,0\n",
      "1301,1\n",
      "1302,0\n",
      "1303,1\n",
      "1304,1\n",
      "1305,0\n",
      "1306,1\n",
      "1307,1\n",
      "1308,1\n",
      "1309,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kaggle submission - .csv file with columns 'PassengerId', 'Survived'\n",
    "myFile = open('titanic_submission1.csv', 'w')\n",
    "writer = csv.writer(myFile)\n",
    "writer.writerow(['PassengerId', 'Survived'])\n",
    "for data_list in titanic_submission:\n",
    "    writer.writerow(data_list)\n",
    "myFile.close()\n",
    "myFile = open('titanic_submission1.csv', 'r')\n",
    "print(\"The content of the csv file is:\")\n",
    "print(myFile.read())\n",
    "myFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "587aa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_submission_test = load_csv('titanic_submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73fe5632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PassengerId', 'Survived'],\n",
       " ['892', '0'],\n",
       " ['893', '0'],\n",
       " ['894', '0'],\n",
       " ['895', '0'],\n",
       " ['896', '0'],\n",
       " ['897', '0'],\n",
       " ['898', '1'],\n",
       " ['899', '0'],\n",
       " ['900', '1'],\n",
       " ['901', '0'],\n",
       " ['902', '1'],\n",
       " ['903', '0'],\n",
       " ['904', '1'],\n",
       " ['905', '0'],\n",
       " ['906', '1'],\n",
       " ['907', '1'],\n",
       " ['908', '0'],\n",
       " ['909', '0'],\n",
       " ['910', '0'],\n",
       " ['911', '1'],\n",
       " ['912', '0'],\n",
       " ['913', '0'],\n",
       " ['914', '0'],\n",
       " ['915', '1'],\n",
       " ['916', '1'],\n",
       " ['917', '0'],\n",
       " ['918', '1'],\n",
       " ['919', '1'],\n",
       " ['920', '0'],\n",
       " ['921', '0'],\n",
       " ['922', '0'],\n",
       " ['923', '0'],\n",
       " ['924', '1'],\n",
       " ['925', '1'],\n",
       " ['926', '1'],\n",
       " ['927', '1'],\n",
       " ['928', '0'],\n",
       " ['929', '1'],\n",
       " ['930', '0'],\n",
       " ['931', '0'],\n",
       " ['932', '0'],\n",
       " ['933', '1'],\n",
       " ['934', '0'],\n",
       " ['935', '1'],\n",
       " ['936', '1'],\n",
       " ['937', '0'],\n",
       " ['938', '0'],\n",
       " ['939', '1'],\n",
       " ['940', '1'],\n",
       " ['941', '1'],\n",
       " ['942', '1'],\n",
       " ['943', '0'],\n",
       " ['944', '0'],\n",
       " ['945', '1'],\n",
       " ['946', '0'],\n",
       " ['947', '0'],\n",
       " ['948', '0'],\n",
       " ['949', '0'],\n",
       " ['950', '0'],\n",
       " ['951', '1'],\n",
       " ['952', '0'],\n",
       " ['953', '0'],\n",
       " ['954', '0'],\n",
       " ['955', '1'],\n",
       " ['956', '0'],\n",
       " ['957', '0'],\n",
       " ['958', '1'],\n",
       " ['959', '0'],\n",
       " ['960', '0'],\n",
       " ['961', '1'],\n",
       " ['962', '1'],\n",
       " ['963', '0'],\n",
       " ['964', '1'],\n",
       " ['965', '1'],\n",
       " ['966', '1'],\n",
       " ['967', '0'],\n",
       " ['968', '1'],\n",
       " ['969', '0'],\n",
       " ['970', '0'],\n",
       " ['971', '1'],\n",
       " ['972', '0'],\n",
       " ['973', '0'],\n",
       " ['974', '0'],\n",
       " ['975', '1'],\n",
       " ['976', '1'],\n",
       " ['977', '0'],\n",
       " ['978', '1'],\n",
       " ['979', '1'],\n",
       " ['980', '0'],\n",
       " ['981', '0'],\n",
       " ['982', '1'],\n",
       " ['983', '1'],\n",
       " ['984', '1'],\n",
       " ['985', '1'],\n",
       " ['986', '0'],\n",
       " ['987', '0'],\n",
       " ['988', '1'],\n",
       " ['989', '0'],\n",
       " ['990', '1'],\n",
       " ['991', '0'],\n",
       " ['992', '1'],\n",
       " ['993', '0'],\n",
       " ['994', '0'],\n",
       " ['995', '0'],\n",
       " ['996', '0'],\n",
       " ['997', '0'],\n",
       " ['998', '0'],\n",
       " ['999', '0'],\n",
       " ['1000', '0'],\n",
       " ['1001', '1'],\n",
       " ['1002', '0'],\n",
       " ['1003', '1'],\n",
       " ['1004', '1'],\n",
       " ['1005', '0'],\n",
       " ['1006', '1'],\n",
       " ['1007', '0'],\n",
       " ['1008', '0'],\n",
       " ['1009', '1'],\n",
       " ['1010', '1'],\n",
       " ['1011', '1'],\n",
       " ['1012', '1'],\n",
       " ['1013', '1'],\n",
       " ['1014', '1'],\n",
       " ['1015', '0'],\n",
       " ['1016', '0'],\n",
       " ['1017', '1'],\n",
       " ['1018', '0'],\n",
       " ['1019', '1'],\n",
       " ['1020', '0'],\n",
       " ['1021', '0'],\n",
       " ['1022', '0'],\n",
       " ['1023', '0'],\n",
       " ['1024', '0'],\n",
       " ['1025', '1'],\n",
       " ['1026', '0'],\n",
       " ['1027', '0'],\n",
       " ['1028', '1'],\n",
       " ['1029', '0'],\n",
       " ['1030', '1'],\n",
       " ['1031', '0'],\n",
       " ['1032', '0'],\n",
       " ['1033', '1'],\n",
       " ['1034', '0'],\n",
       " ['1035', '0'],\n",
       " ['1036', '0'],\n",
       " ['1037', '0'],\n",
       " ['1038', '0'],\n",
       " ['1039', '0'],\n",
       " ['1040', '1'],\n",
       " ['1041', '0'],\n",
       " ['1042', '1'],\n",
       " ['1043', '1'],\n",
       " ['1044', '0'],\n",
       " ['1045', '1'],\n",
       " ['1046', '0'],\n",
       " ['1047', '0'],\n",
       " ['1048', '1'],\n",
       " ['1049', '1'],\n",
       " ['1050', '0'],\n",
       " ['1051', '1'],\n",
       " ['1052', '1'],\n",
       " ['1053', '0'],\n",
       " ['1054', '1'],\n",
       " ['1055', '0'],\n",
       " ['1056', '0'],\n",
       " ['1057', '1'],\n",
       " ['1058', '0'],\n",
       " ['1059', '0'],\n",
       " ['1060', '0'],\n",
       " ['1061', '1'],\n",
       " ['1062', '1'],\n",
       " ['1063', '0'],\n",
       " ['1064', '0'],\n",
       " ['1065', '1'],\n",
       " ['1066', '0'],\n",
       " ['1067', '1'],\n",
       " ['1068', '1'],\n",
       " ['1069', '0'],\n",
       " ['1070', '1'],\n",
       " ['1071', '1'],\n",
       " ['1072', '0'],\n",
       " ['1073', '0'],\n",
       " ['1074', '1'],\n",
       " ['1075', '1'],\n",
       " ['1076', '1'],\n",
       " ['1077', '0'],\n",
       " ['1078', '1'],\n",
       " ['1079', '0'],\n",
       " ['1080', '0'],\n",
       " ['1081', '0'],\n",
       " ['1082', '0'],\n",
       " ['1083', '1'],\n",
       " ['1084', '1'],\n",
       " ['1085', '0'],\n",
       " ['1086', '1'],\n",
       " ['1087', '0'],\n",
       " ['1088', '0'],\n",
       " ['1089', '1'],\n",
       " ['1090', '0'],\n",
       " ['1091', '0'],\n",
       " ['1092', '0'],\n",
       " ['1093', '0'],\n",
       " ['1094', '0'],\n",
       " ['1095', '1'],\n",
       " ['1096', '0'],\n",
       " ['1097', '1'],\n",
       " ['1098', '1'],\n",
       " ['1099', '0'],\n",
       " ['1100', '1'],\n",
       " ['1101', '0'],\n",
       " ['1102', '0'],\n",
       " ['1103', '0'],\n",
       " ['1104', '1'],\n",
       " ['1105', '1'],\n",
       " ['1106', '0'],\n",
       " ['1107', '0'],\n",
       " ['1108', '0'],\n",
       " ['1109', '0'],\n",
       " ['1110', '1'],\n",
       " ['1111', '0'],\n",
       " ['1112', '1'],\n",
       " ['1113', '0'],\n",
       " ['1114', '1'],\n",
       " ['1115', '0'],\n",
       " ['1116', '1'],\n",
       " ['1117', '0'],\n",
       " ['1118', '0'],\n",
       " ['1119', '1'],\n",
       " ['1120', '0'],\n",
       " ['1121', '0'],\n",
       " ['1122', '1'],\n",
       " ['1123', '1'],\n",
       " ['1124', '0'],\n",
       " ['1125', '0'],\n",
       " ['1126', '0'],\n",
       " ['1127', '0'],\n",
       " ['1128', '0'],\n",
       " ['1129', '0'],\n",
       " ['1130', '1'],\n",
       " ['1131', '1'],\n",
       " ['1132', '1'],\n",
       " ['1133', '1'],\n",
       " ['1134', '0'],\n",
       " ['1135', '1'],\n",
       " ['1136', '0'],\n",
       " ['1137', '0'],\n",
       " ['1138', '1'],\n",
       " ['1139', '0'],\n",
       " ['1140', '1'],\n",
       " ['1141', '0'],\n",
       " ['1142', '1'],\n",
       " ['1143', '0'],\n",
       " ['1144', '0'],\n",
       " ['1145', '0'],\n",
       " ['1146', '0'],\n",
       " ['1147', '0'],\n",
       " ['1148', '1'],\n",
       " ['1149', '0'],\n",
       " ['1150', '1'],\n",
       " ['1151', '0'],\n",
       " ['1152', '1'],\n",
       " ['1153', '0'],\n",
       " ['1154', '1'],\n",
       " ['1155', '1'],\n",
       " ['1156', '0'],\n",
       " ['1157', '1'],\n",
       " ['1158', '0'],\n",
       " ['1159', '1'],\n",
       " ['1160', '0'],\n",
       " ['1161', '0'],\n",
       " ['1162', '1'],\n",
       " ['1163', '0'],\n",
       " ['1164', '1'],\n",
       " ['1165', '1'],\n",
       " ['1166', '1'],\n",
       " ['1167', '1'],\n",
       " ['1168', '0'],\n",
       " ['1169', '0'],\n",
       " ['1170', '0'],\n",
       " ['1171', '0'],\n",
       " ['1172', '1'],\n",
       " ['1173', '1'],\n",
       " ['1174', '1'],\n",
       " ['1175', '1'],\n",
       " ['1176', '1'],\n",
       " ['1177', '0'],\n",
       " ['1178', '0'],\n",
       " ['1179', '1'],\n",
       " ['1180', '1'],\n",
       " ['1181', '0'],\n",
       " ['1182', '1'],\n",
       " ['1183', '1'],\n",
       " ['1184', '1'],\n",
       " ['1185', '0'],\n",
       " ['1186', '0'],\n",
       " ['1187', '0'],\n",
       " ['1188', '1'],\n",
       " ['1189', '1'],\n",
       " ['1190', '1'],\n",
       " ['1191', '0'],\n",
       " ['1192', '0'],\n",
       " ['1193', '0'],\n",
       " ['1194', '0'],\n",
       " ['1195', '0'],\n",
       " ['1196', '0'],\n",
       " ['1197', '1'],\n",
       " ['1198', '0'],\n",
       " ['1199', '0'],\n",
       " ['1200', '0'],\n",
       " ['1201', '0'],\n",
       " ['1202', '0'],\n",
       " ['1203', '0'],\n",
       " ['1204', '0'],\n",
       " ['1205', '1'],\n",
       " ['1206', '1'],\n",
       " ['1207', '1'],\n",
       " ['1208', '0'],\n",
       " ['1209', '0'],\n",
       " ['1210', '0'],\n",
       " ['1211', '0'],\n",
       " ['1212', '0'],\n",
       " ['1213', '0'],\n",
       " ['1214', '0'],\n",
       " ['1215', '0'],\n",
       " ['1216', '1'],\n",
       " ['1217', '0'],\n",
       " ['1218', '1'],\n",
       " ['1219', '1'],\n",
       " ['1220', '0'],\n",
       " ['1221', '0'],\n",
       " ['1222', '1'],\n",
       " ['1223', '0'],\n",
       " ['1224', '1'],\n",
       " ['1225', '1'],\n",
       " ['1226', '0'],\n",
       " ['1227', '0'],\n",
       " ['1228', '0'],\n",
       " ['1229', '0'],\n",
       " ['1230', '0'],\n",
       " ['1231', '0'],\n",
       " ['1232', '0'],\n",
       " ['1233', '0'],\n",
       " ['1234', '1'],\n",
       " ['1235', '1'],\n",
       " ['1236', '0'],\n",
       " ['1237', '1'],\n",
       " ['1238', '0'],\n",
       " ['1239', '1'],\n",
       " ['1240', '0'],\n",
       " ['1241', '1'],\n",
       " ['1242', '1'],\n",
       " ['1243', '0'],\n",
       " ['1244', '1'],\n",
       " ['1245', '0'],\n",
       " ['1246', '1'],\n",
       " ['1247', '0'],\n",
       " ['1248', '1'],\n",
       " ['1249', '1'],\n",
       " ['1250', '0'],\n",
       " ['1251', '1'],\n",
       " ['1252', '0'],\n",
       " ['1253', '1'],\n",
       " ['1254', '1'],\n",
       " ['1255', '0'],\n",
       " ['1256', '1'],\n",
       " ['1257', '0'],\n",
       " ['1258', '1'],\n",
       " ['1259', '1'],\n",
       " ['1260', '1'],\n",
       " ['1261', '0'],\n",
       " ['1262', '0'],\n",
       " ['1263', '1'],\n",
       " ['1264', '0'],\n",
       " ['1265', '0'],\n",
       " ['1266', '1'],\n",
       " ['1267', '1'],\n",
       " ['1268', '0'],\n",
       " ['1269', '0'],\n",
       " ['1270', '0'],\n",
       " ['1271', '0'],\n",
       " ['1272', '1'],\n",
       " ['1273', '0'],\n",
       " ['1274', '0'],\n",
       " ['1275', '1'],\n",
       " ['1276', '1'],\n",
       " ['1277', '1'],\n",
       " ['1278', '0'],\n",
       " ['1279', '0'],\n",
       " ['1280', '0'],\n",
       " ['1281', '0'],\n",
       " ['1282', '1'],\n",
       " ['1283', '1'],\n",
       " ['1284', '1'],\n",
       " ['1285', '0'],\n",
       " ['1286', '0'],\n",
       " ['1287', '1'],\n",
       " ['1288', '0'],\n",
       " ['1289', '1'],\n",
       " ['1290', '0'],\n",
       " ['1291', '0'],\n",
       " ['1292', '1'],\n",
       " ['1293', '0'],\n",
       " ['1294', '1'],\n",
       " ['1295', '1'],\n",
       " ['1296', '0'],\n",
       " ['1297', '0'],\n",
       " ['1298', '0'],\n",
       " ['1299', '0'],\n",
       " ['1300', '0'],\n",
       " ['1301', '1'],\n",
       " ['1302', '0'],\n",
       " ['1303', '1'],\n",
       " ['1304', '1'],\n",
       " ['1305', '0'],\n",
       " ['1306', '1'],\n",
       " ['1307', '1'],\n",
       " ['1308', '1'],\n",
       " ['1309', '0']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_submission_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c247842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
